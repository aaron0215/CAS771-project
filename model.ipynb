{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrBtow5tmfKA",
        "outputId": "762a92a3-737c-47cb-a8cd-ae6f1ebee189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR7R1kyudh7O",
        "outputId": "edc7f0ff-6c0b-45fc-d0f8-1f88c23ad4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchsummary numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "ge9VDfwrpkto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_map = [\n",
        "    {\n",
        "        \"train\": \"Model1/model1_train.pth\",\n",
        "        \"test\": \"Model1/model1_test.pth\",\n",
        "        \"classes\": {0: 0, 40: 1, 10: 2, 20: 3, 30: 4}\n",
        "    },\n",
        "    {\n",
        "        \"train\": \"Model2/model2_train.pth\",\n",
        "        \"test\": \"Model2/model2_test.pth\",\n",
        "        \"classes\": {1: 0, 41: 1, 11: 2, 21: 3, 31: 4}\n",
        "    },\n",
        "    {\n",
        "        \"train\": \"Model3/model3_train.pth\",\n",
        "        \"test\": \"Model3/model3_test.pth\",\n",
        "        \"classes\": {32: 0, 2: 1, 42: 2, 12: 3, 22: 4}\n",
        "    }\n",
        "]\n",
        "model_folder = './drive/MyDrive/TaskA/Task1_data'"
      ],
      "metadata": {
        "id": "J-gAlCCUl4pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task2_file_map = [\n",
        "    {\n",
        "        \"train\": \"train_dataB_model_1.pth\",\n",
        "        \"test\": \"val_dataB_model_1.pth\",\n",
        "        \"classes\": {34: 0, 137: 1, 159: 2, 173: 3, 201: 4}\n",
        "    },\n",
        "    {\n",
        "        \"train\": \"train_dataB_model_2.pth\",\n",
        "        \"test\": \"val_dataB_model_2.pth\",\n",
        "        \"classes\": {24: 0, 34: 1, 80: 2, 135: 3, 202: 4}\n",
        "    },\n",
        "    {\n",
        "        \"train\": \"train_dataB_model_3.pth\",\n",
        "        \"test\": \"val_dataB_model_3.pth\",\n",
        "        \"classes\": {124: 0, 125: 1, 130: 2, 173: 3, 202: 4}\n",
        "    }\n",
        "]\n",
        "model_folder = './drive/MyDrive/Task2_data'"
      ],
      "metadata": {
        "id": "I6OQbyQ-PHW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_images(data, labels):\n",
        "    unique_labels = np.unique(labels)\n",
        "    num_classes = len(unique_labels)\n",
        "    fig, axes = plt.subplots(1, num_classes, figsize=(15, 5))\n",
        "    for i, label in enumerate(unique_labels):\n",
        "        idx = np.where(labels == label)[0][0]\n",
        "        image = data[idx]\n",
        "        axes[i].imshow(image.astype(np.uint8))\n",
        "        axes[i].set_title(f'Label: {label}')\n",
        "        axes[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def load_pth(file_path):\n",
        "    raw_data = torch.load(file_path)\n",
        "    return raw_data['data'].numpy(), raw_data['labels'].numpy()\n",
        "\n",
        "test_train_data, test_train_labels = load_pth('./drive/MyDrive/Task2_data/train_dataB_model_1.pth')\n",
        "# test_train_data, test_train_labels = load_pth('./drive/MyDrive/TaskA/Task1_data/Model1/model1_train.pth')\n",
        "print('Displaying images for all classes:')\n",
        "display_images(test_train_data, test_train_labels)"
      ],
      "metadata": {
        "id": "Oqy95mPLUiC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n",
        "    # AutoAugment(AutoAugmentPolicy.CIFAR10),  # 自动增强\n",
        "    # transforms.ColorJitter(\n",
        "    #     brightness=0.2,\n",
        "    #     contrast=0.2,\n",
        "    #     saturation=0.2,\n",
        "    #     hue=0.1\n",
        "    # ),\n",
        "    # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
        "  ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
        "])"
      ],
      "metadata": {
        "id": "mU6p-87VlXif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            # Check tensor shape and permute if necessary\n",
        "            if image.dim() == 3:  # If shape is [C, H, W]\n",
        "                if image.size(0) > 4:  # If first dimension > 4, it's likely [H, W, C]\n",
        "                    image = image.permute(2, 0, 1)  # Convert to [C, H, W]\n",
        "            elif image.dim() == 2:  # If grayscale\n",
        "                image = image.unsqueeze(0)  # Add channel dimension\n",
        "\n",
        "            # Ensure we have 3 channels for RGB\n",
        "            if image.size(0) != 3:\n",
        "                # If it's a different number of channels, reshape to 3 channels\n",
        "                image = image.view(3, 32, 32)\n",
        "\n",
        "            image = transforms.functional.to_pil_image(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "cGb7mWzBlMh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(file,batch_size=32):\n",
        "    train_path = os.path.join(model_folder, file[\"train\"])\n",
        "    test_path = os.path.join(model_folder, file[\"test\"])\n",
        "\n",
        "    # load data\n",
        "    train_data = torch.load(train_path)\n",
        "    data = train_data['data']\n",
        "    labels = train_data['labels']\n",
        "    if isinstance(labels[0], torch.Tensor):\n",
        "      train_labels_tensor = torch.tensor([file['classes'][label.item()] for label in labels], dtype=torch.long)\n",
        "    else:\n",
        "      train_labels_tensor = torch.tensor([file['classes'][label] for label in labels], dtype=torch.long)\n",
        "    # train_labels_tensor = torch.tensor([file['classes'][label] for label in labels], dtype=torch.long)\n",
        "    train_dataset = CustomDataset(data, train_labels_tensor, transform=train_transform)\n",
        "\n",
        "    test_data = torch.load(test_path)\n",
        "    data = test_data['data']\n",
        "    labels = test_data['labels']\n",
        "    if isinstance(labels[0], torch.Tensor):\n",
        "      test_labels_tensor = torch.tensor([file['classes'][label.item()] for label in labels], dtype=torch.long)\n",
        "    else:\n",
        "      test_labels_tensor = torch.tensor([file['classes'][label] for label in labels], dtype=torch.long)\n",
        "    # test_labels_tensor = torch.tensor([file['classes'][label] for label in labels], dtype=torch.long)\n",
        "    test_dataset = CustomDataset(data, test_labels_tensor, transform=train_transform)\n",
        "\n",
        "    # generate data loader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "08DkBZTrqOPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, num_classes=5, dropout_rate=0.2):\n",
        "      super(CNN, self).__init__()\n",
        "\n",
        "      self.block1 = nn.Sequential(\n",
        "          nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.MaxPool2d(2),\n",
        "          nn.Dropout2d(0.2)\n",
        "      )\n",
        "\n",
        "      self.block2 = nn.Sequential(\n",
        "          nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "          nn.BatchNorm2d(128),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "          nn.BatchNorm2d(128),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.MaxPool2d(2),\n",
        "          nn.Dropout2d(0.2)\n",
        "      )\n",
        "\n",
        "      self.block3 = nn.Sequential(\n",
        "          nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "          nn.BatchNorm2d(256),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.MaxPool2d(2),\n",
        "          nn.Dropout2d(0.2)\n",
        "      )\n",
        "\n",
        "      self.classifier = nn.Sequential(\n",
        "          nn.Linear(256 * 4 * 4, 128),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Dropout(dropout_rate),\n",
        "          nn.Linear(128, num_classes)\n",
        "      )\n",
        "      # Initialize weights properly\n",
        "      self._initialize_weights()\n",
        "\n",
        "  def _initialize_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, 0, 0.01)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  # connect layers\n",
        "  def forward(self, x):\n",
        "      x = self.block1(x)\n",
        "      x = self.block2(x)\n",
        "      x = self.block3(x)\n",
        "      x = x.view(x.size(0), -1)\n",
        "      x = self.classifier(x)\n",
        "      # x = F.log_softmax(self.fc3(x), dim=1)\n",
        "      return x"
      ],
      "metadata": {
        "id": "3SCpBTH1xgiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  device = torch.device(\"cpu\")\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  elif torch.backends.mps.is_built() and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "  print(device)\n",
        "\n",
        "  batch_size = 32\n",
        "  learning_rate = 0.001\n",
        "  weight_decay = 0.0001\n",
        "\n",
        "  #testA: file_map\n",
        "  #testB: task2_file_map\n",
        "  for i, file in enumerate(task2_file_map):\n",
        "    net = CNN(num_classes=5).to(device)\n",
        "    train_loader, test_loader = create_dataloaders(file)\n",
        "     # use cross entropy loss and adam optimizer\n",
        "    loss_function = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # training\n",
        "    epochs = 100\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      running_loss = 0.0\n",
        "      loss_history = []\n",
        "      net.to(device)\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "      epoch_loss = running_loss / len(train_loader.dataset)\n",
        "      loss_history.append(epoch_loss)\n",
        "      print(f\"Epoch {epoch + 1}/{epochs} Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1wFo9xmxnx8",
        "outputId": "bc846bfe-bde9-45c4-a938-67368792da12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-e0807349c826>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train_data = torch.load(train_path)\n",
            "<ipython-input-8-e0807349c826>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_data = torch.load(test_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 Loss: 1.5376\n",
            "Epoch 2/100 Loss: 1.5116\n",
            "Epoch 3/100 Loss: 1.5150\n",
            "Epoch 4/100 Loss: 1.5076\n",
            "Epoch 5/100 Loss: 1.4931\n",
            "Epoch 6/100 Loss: 1.4893\n",
            "Epoch 7/100 Loss: 1.4910\n",
            "Epoch 8/100 Loss: 1.4726\n",
            "Epoch 9/100 Loss: 1.4650\n",
            "Epoch 10/100 Loss: 1.4607\n",
            "Epoch 11/100 Loss: 1.4581\n",
            "Epoch 12/100 Loss: 1.4482\n",
            "Epoch 13/100 Loss: 1.4452\n",
            "Epoch 14/100 Loss: 1.4257\n",
            "Epoch 15/100 Loss: 1.4259\n",
            "Epoch 16/100 Loss: 1.4265\n",
            "Epoch 17/100 Loss: 1.4324\n",
            "Epoch 18/100 Loss: 1.4150\n",
            "Epoch 19/100 Loss: 1.4214\n",
            "Epoch 20/100 Loss: 1.4263\n",
            "Epoch 21/100 Loss: 1.4248\n",
            "Epoch 22/100 Loss: 1.4148\n",
            "Epoch 23/100 Loss: 1.3990\n",
            "Epoch 24/100 Loss: 1.3922\n",
            "Epoch 25/100 Loss: 1.4065\n",
            "Epoch 26/100 Loss: 1.4151\n",
            "Epoch 27/100 Loss: 1.3985\n",
            "Epoch 28/100 Loss: 1.3930\n",
            "Epoch 29/100 Loss: 1.3902\n",
            "Epoch 30/100 Loss: 1.3848\n",
            "Epoch 31/100 Loss: 1.3874\n",
            "Epoch 32/100 Loss: 1.3786\n",
            "Epoch 33/100 Loss: 1.3850\n",
            "Epoch 34/100 Loss: 1.3863\n",
            "Epoch 35/100 Loss: 1.3567\n",
            "Epoch 36/100 Loss: 1.3738\n",
            "Epoch 37/100 Loss: 1.3556\n",
            "Epoch 38/100 Loss: 1.3694\n",
            "Epoch 39/100 Loss: 1.3618\n",
            "Epoch 40/100 Loss: 1.3590\n",
            "Epoch 41/100 Loss: 1.3495\n",
            "Epoch 42/100 Loss: 1.3536\n",
            "Epoch 43/100 Loss: 1.3478\n",
            "Epoch 44/100 Loss: 1.3327\n",
            "Epoch 45/100 Loss: 1.3322\n",
            "Epoch 46/100 Loss: 1.3455\n",
            "Epoch 47/100 Loss: 1.3266\n",
            "Epoch 48/100 Loss: 1.3207\n",
            "Epoch 49/100 Loss: 1.3175\n",
            "Epoch 50/100 Loss: 1.3255\n",
            "Epoch 51/100 Loss: 1.3240\n",
            "Epoch 52/100 Loss: 1.3142\n",
            "Epoch 53/100 Loss: 1.3189\n",
            "Epoch 54/100 Loss: 1.3256\n",
            "Epoch 55/100 Loss: 1.3263\n",
            "Epoch 56/100 Loss: 1.2998\n",
            "Epoch 57/100 Loss: 1.3143\n",
            "Epoch 58/100 Loss: 1.2967\n",
            "Epoch 59/100 Loss: 1.2879\n",
            "Epoch 60/100 Loss: 1.3093\n",
            "Epoch 61/100 Loss: 1.2935\n",
            "Epoch 62/100 Loss: 1.2761\n",
            "Epoch 63/100 Loss: 1.2820\n",
            "Epoch 64/100 Loss: 1.2850\n",
            "Epoch 65/100 Loss: 1.2940\n",
            "Epoch 66/100 Loss: 1.2678\n",
            "Epoch 67/100 Loss: 1.2696\n",
            "Epoch 68/100 Loss: 1.2672\n",
            "Epoch 69/100 Loss: 1.2627\n",
            "Epoch 70/100 Loss: 1.2722\n",
            "Epoch 71/100 Loss: 1.2613\n",
            "Epoch 72/100 Loss: 1.2748\n",
            "Epoch 73/100 Loss: 1.2693\n",
            "Epoch 74/100 Loss: 1.2518\n",
            "Epoch 75/100 Loss: 1.2538\n",
            "Epoch 76/100 Loss: 1.2672\n",
            "Epoch 77/100 Loss: 1.2337\n",
            "Epoch 78/100 Loss: 1.2478\n",
            "Epoch 79/100 Loss: 1.2619\n",
            "Epoch 80/100 Loss: 1.2468\n",
            "Epoch 81/100 Loss: 1.2269\n",
            "Epoch 82/100 Loss: 1.2428\n",
            "Epoch 83/100 Loss: 1.2340\n",
            "Epoch 84/100 Loss: 1.2472\n",
            "Epoch 85/100 Loss: 1.2327\n",
            "Epoch 86/100 Loss: 1.2267\n",
            "Epoch 87/100 Loss: 1.2288\n",
            "Epoch 88/100 Loss: 1.2291\n",
            "Epoch 89/100 Loss: 1.2135\n",
            "Epoch 90/100 Loss: 1.2367\n",
            "Epoch 91/100 Loss: 1.2307\n",
            "Epoch 92/100 Loss: 1.2359\n",
            "Epoch 93/100 Loss: 1.2079\n",
            "Epoch 94/100 Loss: 1.2095\n",
            "Epoch 95/100 Loss: 1.2028\n",
            "Epoch 96/100 Loss: 1.1993\n",
            "Epoch 97/100 Loss: 1.2033\n",
            "Epoch 98/100 Loss: 1.2070\n",
            "Epoch 99/100 Loss: 1.2119\n",
            "Epoch 100/100 Loss: 1.1978\n",
            "Finished Training\n",
            "Test Accuracy: 61.20%\n",
            "Epoch 1/100 Loss: 1.5500\n",
            "Epoch 2/100 Loss: 1.5198\n",
            "Epoch 3/100 Loss: 1.4952\n",
            "Epoch 4/100 Loss: 1.4749\n",
            "Epoch 5/100 Loss: 1.4611\n",
            "Epoch 6/100 Loss: 1.4671\n",
            "Epoch 7/100 Loss: 1.4466\n",
            "Epoch 8/100 Loss: 1.4504\n",
            "Epoch 9/100 Loss: 1.4253\n",
            "Epoch 10/100 Loss: 1.4352\n",
            "Epoch 11/100 Loss: 1.4285\n",
            "Epoch 12/100 Loss: 1.4204\n",
            "Epoch 13/100 Loss: 1.4060\n",
            "Epoch 14/100 Loss: 1.4029\n",
            "Epoch 15/100 Loss: 1.3985\n",
            "Epoch 16/100 Loss: 1.3724\n",
            "Epoch 17/100 Loss: 1.3737\n",
            "Epoch 18/100 Loss: 1.3723\n",
            "Epoch 19/100 Loss: 1.3672\n",
            "Epoch 20/100 Loss: 1.3518\n",
            "Epoch 21/100 Loss: 1.3445\n",
            "Epoch 22/100 Loss: 1.3388\n",
            "Epoch 23/100 Loss: 1.3303\n",
            "Epoch 24/100 Loss: 1.3280\n",
            "Epoch 25/100 Loss: 1.3287\n",
            "Epoch 26/100 Loss: 1.3399\n",
            "Epoch 27/100 Loss: 1.3097\n",
            "Epoch 28/100 Loss: 1.2962\n",
            "Epoch 29/100 Loss: 1.3023\n",
            "Epoch 30/100 Loss: 1.2941\n",
            "Epoch 31/100 Loss: 1.2877\n",
            "Epoch 32/100 Loss: 1.2863\n",
            "Epoch 33/100 Loss: 1.2698\n",
            "Epoch 34/100 Loss: 1.2812\n",
            "Epoch 35/100 Loss: 1.2633\n",
            "Epoch 36/100 Loss: 1.2559\n",
            "Epoch 37/100 Loss: 1.2500\n",
            "Epoch 38/100 Loss: 1.2569\n",
            "Epoch 39/100 Loss: 1.2309\n",
            "Epoch 40/100 Loss: 1.2431\n",
            "Epoch 41/100 Loss: 1.2262\n",
            "Epoch 42/100 Loss: 1.2352\n",
            "Epoch 43/100 Loss: 1.2063\n",
            "Epoch 44/100 Loss: 1.2362\n",
            "Epoch 45/100 Loss: 1.1956\n",
            "Epoch 46/100 Loss: 1.2100\n",
            "Epoch 47/100 Loss: 1.2205\n",
            "Epoch 48/100 Loss: 1.2126\n",
            "Epoch 49/100 Loss: 1.2086\n",
            "Epoch 50/100 Loss: 1.2013\n",
            "Epoch 51/100 Loss: 1.1770\n",
            "Epoch 52/100 Loss: 1.1804\n",
            "Epoch 53/100 Loss: 1.1778\n",
            "Epoch 54/100 Loss: 1.1769\n",
            "Epoch 55/100 Loss: 1.1913\n",
            "Epoch 56/100 Loss: 1.1645\n",
            "Epoch 57/100 Loss: 1.1855\n",
            "Epoch 58/100 Loss: 1.1718\n",
            "Epoch 59/100 Loss: 1.1496\n",
            "Epoch 60/100 Loss: 1.1462\n",
            "Epoch 61/100 Loss: 1.1670\n",
            "Epoch 62/100 Loss: 1.1707\n",
            "Epoch 63/100 Loss: 1.1738\n",
            "Epoch 64/100 Loss: 1.1447\n",
            "Epoch 65/100 Loss: 1.1481\n",
            "Epoch 66/100 Loss: 1.1326\n",
            "Epoch 67/100 Loss: 1.1507\n",
            "Epoch 68/100 Loss: 1.1391\n",
            "Epoch 69/100 Loss: 1.1206\n",
            "Epoch 70/100 Loss: 1.1293\n",
            "Epoch 71/100 Loss: 1.1294\n",
            "Epoch 72/100 Loss: 1.1196\n",
            "Epoch 73/100 Loss: 1.1127\n",
            "Epoch 74/100 Loss: 1.1119\n",
            "Epoch 75/100 Loss: 1.1192\n",
            "Epoch 76/100 Loss: 1.1044\n",
            "Epoch 77/100 Loss: 1.0914\n",
            "Epoch 78/100 Loss: 1.1135\n",
            "Epoch 79/100 Loss: 1.0972\n",
            "Epoch 80/100 Loss: 1.1027\n",
            "Epoch 81/100 Loss: 1.0888\n",
            "Epoch 82/100 Loss: 1.0667\n",
            "Epoch 83/100 Loss: 1.1105\n",
            "Epoch 84/100 Loss: 1.0931\n",
            "Epoch 85/100 Loss: 1.1077\n",
            "Epoch 86/100 Loss: 1.0709\n",
            "Epoch 87/100 Loss: 1.0782\n",
            "Epoch 88/100 Loss: 1.0837\n",
            "Epoch 89/100 Loss: 1.0694\n",
            "Epoch 90/100 Loss: 1.0724\n",
            "Epoch 91/100 Loss: 1.0698\n",
            "Epoch 92/100 Loss: 1.0667\n",
            "Epoch 93/100 Loss: 1.0687\n",
            "Epoch 94/100 Loss: 1.0665\n",
            "Epoch 95/100 Loss: 1.0591\n",
            "Epoch 96/100 Loss: 1.0502\n",
            "Epoch 97/100 Loss: 1.0526\n",
            "Epoch 98/100 Loss: 1.0609\n",
            "Epoch 99/100 Loss: 1.0512\n",
            "Epoch 100/100 Loss: 1.0417\n",
            "Finished Training\n",
            "Test Accuracy: 75.20%\n",
            "Epoch 1/100 Loss: 1.5901\n",
            "Epoch 2/100 Loss: 1.5595\n",
            "Epoch 3/100 Loss: 1.5501\n",
            "Epoch 4/100 Loss: 1.5351\n",
            "Epoch 5/100 Loss: 1.5322\n",
            "Epoch 6/100 Loss: 1.5290\n",
            "Epoch 7/100 Loss: 1.5124\n",
            "Epoch 8/100 Loss: 1.5051\n",
            "Epoch 9/100 Loss: 1.4899\n",
            "Epoch 10/100 Loss: 1.4836\n",
            "Epoch 11/100 Loss: 1.4757\n",
            "Epoch 12/100 Loss: 1.4869\n",
            "Epoch 13/100 Loss: 1.4746\n",
            "Epoch 14/100 Loss: 1.4625\n",
            "Epoch 15/100 Loss: 1.4522\n",
            "Epoch 16/100 Loss: 1.4796\n",
            "Epoch 17/100 Loss: 1.4510\n",
            "Epoch 18/100 Loss: 1.4551\n",
            "Epoch 19/100 Loss: 1.4478\n",
            "Epoch 20/100 Loss: 1.4417\n",
            "Epoch 21/100 Loss: 1.4516\n",
            "Epoch 22/100 Loss: 1.4483\n",
            "Epoch 23/100 Loss: 1.4332\n",
            "Epoch 24/100 Loss: 1.4401\n",
            "Epoch 25/100 Loss: 1.4283\n",
            "Epoch 26/100 Loss: 1.4413\n",
            "Epoch 27/100 Loss: 1.4305\n",
            "Epoch 28/100 Loss: 1.4260\n",
            "Epoch 29/100 Loss: 1.4206\n",
            "Epoch 30/100 Loss: 1.4241\n",
            "Epoch 31/100 Loss: 1.4228\n",
            "Epoch 32/100 Loss: 1.4137\n",
            "Epoch 33/100 Loss: 1.4153\n",
            "Epoch 34/100 Loss: 1.4164\n",
            "Epoch 35/100 Loss: 1.4300\n",
            "Epoch 36/100 Loss: 1.4159\n",
            "Epoch 37/100 Loss: 1.4056\n",
            "Epoch 38/100 Loss: 1.4142\n",
            "Epoch 39/100 Loss: 1.4080\n",
            "Epoch 40/100 Loss: 1.4124\n",
            "Epoch 41/100 Loss: 1.3958\n",
            "Epoch 42/100 Loss: 1.4049\n",
            "Epoch 43/100 Loss: 1.4053\n",
            "Epoch 44/100 Loss: 1.3906\n",
            "Epoch 45/100 Loss: 1.3865\n",
            "Epoch 46/100 Loss: 1.3819\n",
            "Epoch 47/100 Loss: 1.3841\n",
            "Epoch 48/100 Loss: 1.3899\n",
            "Epoch 49/100 Loss: 1.3937\n",
            "Epoch 50/100 Loss: 1.3917\n",
            "Epoch 51/100 Loss: 1.3827\n",
            "Epoch 52/100 Loss: 1.3759\n",
            "Epoch 53/100 Loss: 1.3624\n",
            "Epoch 54/100 Loss: 1.3704\n",
            "Epoch 55/100 Loss: 1.3805\n",
            "Epoch 56/100 Loss: 1.3675\n",
            "Epoch 57/100 Loss: 1.3734\n",
            "Epoch 58/100 Loss: 1.3615\n",
            "Epoch 59/100 Loss: 1.3456\n",
            "Epoch 60/100 Loss: 1.3642\n",
            "Epoch 61/100 Loss: 1.3613\n",
            "Epoch 62/100 Loss: 1.3495\n",
            "Epoch 63/100 Loss: 1.3525\n",
            "Epoch 64/100 Loss: 1.3454\n",
            "Epoch 65/100 Loss: 1.3416\n",
            "Epoch 66/100 Loss: 1.3427\n",
            "Epoch 67/100 Loss: 1.3360\n",
            "Epoch 68/100 Loss: 1.3536\n",
            "Epoch 69/100 Loss: 1.3484\n",
            "Epoch 70/100 Loss: 1.3309\n",
            "Epoch 71/100 Loss: 1.3295\n",
            "Epoch 72/100 Loss: 1.3307\n",
            "Epoch 73/100 Loss: 1.3144\n",
            "Epoch 74/100 Loss: 1.3289\n",
            "Epoch 75/100 Loss: 1.3067\n",
            "Epoch 76/100 Loss: 1.3215\n",
            "Epoch 77/100 Loss: 1.3218\n",
            "Epoch 78/100 Loss: 1.3146\n",
            "Epoch 79/100 Loss: 1.3132\n",
            "Epoch 80/100 Loss: 1.3114\n",
            "Epoch 81/100 Loss: 1.3269\n",
            "Epoch 82/100 Loss: 1.3023\n",
            "Epoch 83/100 Loss: 1.3014\n",
            "Epoch 84/100 Loss: 1.2902\n",
            "Epoch 85/100 Loss: 1.2969\n",
            "Epoch 86/100 Loss: 1.2897\n",
            "Epoch 87/100 Loss: 1.3071\n",
            "Epoch 88/100 Loss: 1.2925\n",
            "Epoch 89/100 Loss: 1.2913\n",
            "Epoch 90/100 Loss: 1.2892\n",
            "Epoch 91/100 Loss: 1.2889\n",
            "Epoch 92/100 Loss: 1.2811\n",
            "Epoch 93/100 Loss: 1.2770\n",
            "Epoch 94/100 Loss: 1.2607\n",
            "Epoch 95/100 Loss: 1.2733\n",
            "Epoch 96/100 Loss: 1.2787\n",
            "Epoch 97/100 Loss: 1.2823\n",
            "Epoch 98/100 Loss: 1.2737\n",
            "Epoch 99/100 Loss: 1.2733\n",
            "Epoch 100/100 Loss: 1.2772\n",
            "Finished Training\n",
            "Test Accuracy: 56.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_built() and torch.backends.mps.is_available():\n",
        "  device = torch.device(\"mps\")\n",
        "print(device)\n",
        "\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, num_models=3, num_classes=5):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.models = nn.ModuleList([\n",
        "            CNN(num_classes=num_classes) for _ in range(num_models)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, model_idx):\n",
        "        return self.models[model_idx](x)\n",
        "\n",
        "def create_weighted_loss(file_map):\n",
        "    # Calculate weights based on label frequency\n",
        "    label_counts = {}\n",
        "    for file in file_map:\n",
        "        for label in file['classes'].values():\n",
        "            label_counts[label] = label_counts.get(label, 0) + 1\n",
        "\n",
        "    weights = torch.ones(5)\n",
        "    for label, count in label_counts.items():\n",
        "        if count > 1:\n",
        "            weights[label] = 1.0 / count\n",
        "\n",
        "    return nn.CrossEntropyLoss(weight=weights.to(device), label_smoothing=0.1)\n",
        "\n",
        "def train_ensemble_model(ensemble, train_loaders, test_loaders, epochs=100):\n",
        "    criterion = create_weighted_loss(task2_file_map)\n",
        "    optimizers = [\n",
        "        optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "        for model in ensemble.models\n",
        "    ]\n",
        "    schedulers = [\n",
        "        optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
        "        )\n",
        "        for optimizer in optimizers\n",
        "    ]\n",
        "\n",
        "    best_accuracies = [0.0] * len(ensemble.models)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        ensemble.train()\n",
        "        for model_idx in range(len(ensemble.models)):\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for inputs, labels in train_loaders[model_idx]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizers[model_idx].zero_grad()\n",
        "                outputs = ensemble(inputs, model_idx)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(ensemble.models[model_idx].parameters(), 1.0)\n",
        "\n",
        "                optimizers[model_idx].step()\n",
        "                schedulers[model_idx].step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            epoch_loss = running_loss / len(train_loaders[model_idx].dataset)\n",
        "            epoch_acc = 100. * correct / total\n",
        "\n",
        "            # Validation phase\n",
        "            val_acc = evaluate_model(\n",
        "                ensemble, model_idx, test_loaders[model_idx], criterion, device\n",
        "            )\n",
        "\n",
        "            print(f'Model {model_idx + 1} - Epoch {epoch + 1}/{epochs} - Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    return ensemble\n",
        "\n",
        "def evaluate_model(ensemble, model_idx, loader, criterion, device):\n",
        "    ensemble.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = ensemble(inputs, model_idx)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    batch_size = 128\n",
        "    ensemble = EnsembleModel().to(device)\n",
        "\n",
        "    # Create dataloaders for each model\n",
        "    train_loaders = []\n",
        "    test_loaders = []\n",
        "    for file in task2_file_map:\n",
        "        train_loader, test_loader = create_dataloaders(file, batch_size=batch_size)\n",
        "        train_loaders.append(train_loader)\n",
        "        test_loaders.append(test_loader)\n",
        "\n",
        "    # Train ensemble\n",
        "    ensemble = train_ensemble_model(\n",
        "        ensemble, train_loaders, test_loaders, epochs=100\n",
        "    )\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\nFinal Test Accuracies:\")\n",
        "    total_acc = 0\n",
        "    for i in range(len(ensemble.models)):\n",
        "        acc = evaluate_model(ensemble, i, test_loaders[i], None, device)\n",
        "        total_acc += acc\n",
        "        print(f\"Model {i + 1}: {acc:.2f}%\")\n",
        "\n",
        "    avg_acc = total_acc / len(ensemble.models)\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Average Model Accuracy: {avg_acc:.2f}%\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxP9fLbC5Xzu",
        "outputId": "c364c15e-0c67-4d8f-a9e4-4e48b9d71222"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Using device: cuda\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-e0807349c826>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train_data = torch.load(train_path)\n",
            "<ipython-input-7-e0807349c826>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_data = torch.load(test_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 - Epoch 1/100 - Loss: 1.5599\n",
            "Model 2 - Epoch 1/100 - Loss: 1.5921\n",
            "Model 3 - Epoch 1/100 - Loss: 1.6018\n",
            "Model 1 - Epoch 2/100 - Loss: 1.5128\n",
            "Model 2 - Epoch 2/100 - Loss: 1.5899\n",
            "Model 3 - Epoch 2/100 - Loss: 1.5908\n",
            "Model 1 - Epoch 3/100 - Loss: 1.4869\n",
            "Model 2 - Epoch 3/100 - Loss: 1.5374\n",
            "Model 3 - Epoch 3/100 - Loss: 1.5873\n",
            "Model 1 - Epoch 4/100 - Loss: 1.4878\n",
            "Model 2 - Epoch 4/100 - Loss: 1.5135\n",
            "Model 3 - Epoch 4/100 - Loss: 1.5525\n",
            "Model 1 - Epoch 5/100 - Loss: 1.4690\n",
            "Model 2 - Epoch 5/100 - Loss: 1.4929\n",
            "Model 3 - Epoch 5/100 - Loss: 1.5321\n",
            "Model 1 - Epoch 6/100 - Loss: 1.4556\n",
            "Model 2 - Epoch 6/100 - Loss: 1.4910\n",
            "Model 3 - Epoch 6/100 - Loss: 1.6139\n",
            "Model 1 - Epoch 7/100 - Loss: 1.4473\n",
            "Model 2 - Epoch 7/100 - Loss: 1.5905\n",
            "Model 3 - Epoch 7/100 - Loss: 1.5411\n",
            "Model 1 - Epoch 8/100 - Loss: 1.4466\n",
            "Model 2 - Epoch 8/100 - Loss: 1.4964\n",
            "Model 3 - Epoch 8/100 - Loss: 1.4910\n",
            "Model 1 - Epoch 9/100 - Loss: 1.4336\n",
            "Model 2 - Epoch 9/100 - Loss: 1.4745\n",
            "Model 3 - Epoch 9/100 - Loss: 1.4866\n",
            "Model 1 - Epoch 10/100 - Loss: 1.4153\n",
            "Model 2 - Epoch 10/100 - Loss: 1.4429\n",
            "Model 3 - Epoch 10/100 - Loss: 1.4749\n",
            "Model 1 - Epoch 11/100 - Loss: 1.4016\n",
            "Model 2 - Epoch 11/100 - Loss: 1.4435\n",
            "Model 3 - Epoch 11/100 - Loss: 1.4511\n",
            "Model 1 - Epoch 12/100 - Loss: 1.4073\n",
            "Model 2 - Epoch 12/100 - Loss: 1.4315\n",
            "Model 3 - Epoch 12/100 - Loss: 1.4437\n",
            "Model 1 - Epoch 13/100 - Loss: 1.4108\n",
            "Model 2 - Epoch 13/100 - Loss: 1.4565\n",
            "Model 3 - Epoch 13/100 - Loss: 1.4823\n",
            "Model 1 - Epoch 14/100 - Loss: 1.4516\n",
            "Model 2 - Epoch 14/100 - Loss: 1.4340\n",
            "Model 3 - Epoch 14/100 - Loss: 1.4751\n",
            "Model 1 - Epoch 15/100 - Loss: 1.4316\n",
            "Model 2 - Epoch 15/100 - Loss: 1.4324\n",
            "Model 3 - Epoch 15/100 - Loss: 1.4576\n",
            "Model 1 - Epoch 16/100 - Loss: 1.4084\n",
            "Model 2 - Epoch 16/100 - Loss: 1.4305\n",
            "Model 3 - Epoch 16/100 - Loss: 1.4467\n",
            "Model 1 - Epoch 17/100 - Loss: 1.3907\n",
            "Model 2 - Epoch 17/100 - Loss: 1.4082\n",
            "Model 3 - Epoch 17/100 - Loss: 1.4627\n",
            "Model 1 - Epoch 18/100 - Loss: 1.3843\n",
            "Model 2 - Epoch 18/100 - Loss: 1.3707\n",
            "Model 3 - Epoch 18/100 - Loss: 1.4142\n",
            "Model 1 - Epoch 19/100 - Loss: 1.3842\n",
            "Model 2 - Epoch 19/100 - Loss: 1.3493\n",
            "Model 3 - Epoch 19/100 - Loss: 1.4083\n",
            "Model 1 - Epoch 20/100 - Loss: 1.3626\n",
            "Model 2 - Epoch 20/100 - Loss: 1.3350\n",
            "Model 3 - Epoch 20/100 - Loss: 1.3866\n",
            "Model 1 - Epoch 21/100 - Loss: 1.3659\n",
            "Model 2 - Epoch 21/100 - Loss: 1.3005\n",
            "Model 3 - Epoch 21/100 - Loss: 1.3796\n",
            "Model 1 - Epoch 22/100 - Loss: 1.3562\n",
            "Model 2 - Epoch 22/100 - Loss: 1.2916\n",
            "Model 3 - Epoch 22/100 - Loss: 1.3711\n",
            "Model 1 - Epoch 23/100 - Loss: 1.3610\n",
            "Model 2 - Epoch 23/100 - Loss: 1.2813\n",
            "Model 3 - Epoch 23/100 - Loss: 1.3755\n",
            "Model 1 - Epoch 24/100 - Loss: 1.3445\n",
            "Model 2 - Epoch 24/100 - Loss: 1.2921\n",
            "Model 3 - Epoch 24/100 - Loss: 1.3611\n",
            "Model 1 - Epoch 25/100 - Loss: 1.3271\n",
            "Model 2 - Epoch 25/100 - Loss: 1.3318\n",
            "Model 3 - Epoch 25/100 - Loss: 1.3878\n",
            "Model 1 - Epoch 26/100 - Loss: 1.3714\n",
            "Model 2 - Epoch 26/100 - Loss: 1.3262\n",
            "Model 3 - Epoch 26/100 - Loss: 1.4031\n",
            "Model 1 - Epoch 27/100 - Loss: 1.3823\n",
            "Model 2 - Epoch 27/100 - Loss: 1.3005\n",
            "Model 3 - Epoch 27/100 - Loss: 1.3789\n",
            "Model 1 - Epoch 28/100 - Loss: 1.3826\n",
            "Model 2 - Epoch 28/100 - Loss: 1.2847\n",
            "Model 3 - Epoch 28/100 - Loss: 1.3818\n",
            "Model 1 - Epoch 29/100 - Loss: 1.3828\n",
            "Model 2 - Epoch 29/100 - Loss: 1.2736\n",
            "Model 3 - Epoch 29/100 - Loss: 1.3660\n",
            "Model 1 - Epoch 30/100 - Loss: 1.3792\n",
            "Model 2 - Epoch 30/100 - Loss: 1.2419\n",
            "Model 3 - Epoch 30/100 - Loss: 1.3618\n",
            "Model 1 - Epoch 31/100 - Loss: 1.3595\n",
            "Model 2 - Epoch 31/100 - Loss: 1.2354\n",
            "Model 3 - Epoch 31/100 - Loss: 1.3666\n",
            "Model 1 - Epoch 32/100 - Loss: 1.3588\n",
            "Model 2 - Epoch 32/100 - Loss: 1.2140\n",
            "Model 3 - Epoch 32/100 - Loss: 1.3509\n",
            "Model 1 - Epoch 33/100 - Loss: 1.3795\n",
            "Model 2 - Epoch 33/100 - Loss: 1.1959\n",
            "Model 3 - Epoch 33/100 - Loss: 1.3433\n",
            "Model 1 - Epoch 34/100 - Loss: 1.3578\n",
            "Model 2 - Epoch 34/100 - Loss: 1.1635\n",
            "Model 3 - Epoch 34/100 - Loss: 1.3106\n",
            "Model 1 - Epoch 35/100 - Loss: 1.3317\n",
            "Model 2 - Epoch 35/100 - Loss: 1.1646\n",
            "Model 3 - Epoch 35/100 - Loss: 1.3067\n",
            "Model 1 - Epoch 36/100 - Loss: 1.3508\n",
            "Model 2 - Epoch 36/100 - Loss: 1.1287\n",
            "Model 3 - Epoch 36/100 - Loss: 1.2942\n",
            "Model 1 - Epoch 37/100 - Loss: 1.3326\n",
            "Model 2 - Epoch 37/100 - Loss: 1.1280\n",
            "Model 3 - Epoch 37/100 - Loss: 1.2774\n",
            "Model 1 - Epoch 38/100 - Loss: 1.3098\n",
            "Model 2 - Epoch 38/100 - Loss: 1.1229\n",
            "Model 3 - Epoch 38/100 - Loss: 1.2933\n",
            "Model 1 - Epoch 39/100 - Loss: 1.3285\n",
            "Model 2 - Epoch 39/100 - Loss: 1.1045\n",
            "Model 3 - Epoch 39/100 - Loss: 1.2760\n",
            "Model 1 - Epoch 40/100 - Loss: 1.3104\n",
            "Model 2 - Epoch 40/100 - Loss: 1.1083\n",
            "Model 3 - Epoch 40/100 - Loss: 1.2539\n",
            "Model 1 - Epoch 41/100 - Loss: 1.2954\n",
            "Model 2 - Epoch 41/100 - Loss: 1.0748\n",
            "Model 3 - Epoch 41/100 - Loss: 1.2755\n",
            "Model 1 - Epoch 42/100 - Loss: 1.2998\n",
            "Model 2 - Epoch 42/100 - Loss: 1.0750\n",
            "Model 3 - Epoch 42/100 - Loss: 1.2343\n",
            "Model 1 - Epoch 43/100 - Loss: 1.2938\n",
            "Model 2 - Epoch 43/100 - Loss: 1.0755\n",
            "Model 3 - Epoch 43/100 - Loss: 1.2454\n",
            "Model 1 - Epoch 44/100 - Loss: 1.2903\n",
            "Model 2 - Epoch 44/100 - Loss: 1.0663\n",
            "Model 3 - Epoch 44/100 - Loss: 1.2314\n",
            "Model 1 - Epoch 45/100 - Loss: 1.2814\n",
            "Model 2 - Epoch 45/100 - Loss: 1.0491\n",
            "Model 3 - Epoch 45/100 - Loss: 1.2285\n",
            "Model 1 - Epoch 46/100 - Loss: 1.2822\n",
            "Model 2 - Epoch 46/100 - Loss: 1.0463\n",
            "Model 3 - Epoch 46/100 - Loss: 1.2226\n",
            "Model 1 - Epoch 47/100 - Loss: 1.2683\n",
            "Model 2 - Epoch 47/100 - Loss: 1.0373\n",
            "Model 3 - Epoch 47/100 - Loss: 1.2163\n",
            "Model 1 - Epoch 48/100 - Loss: 1.2817\n",
            "Model 2 - Epoch 48/100 - Loss: 1.0462\n",
            "Model 3 - Epoch 48/100 - Loss: 1.2083\n",
            "Model 1 - Epoch 49/100 - Loss: 1.2764\n",
            "Model 2 - Epoch 49/100 - Loss: 1.0287\n",
            "Model 3 - Epoch 49/100 - Loss: 1.2100\n",
            "Model 1 - Epoch 50/100 - Loss: 1.2703\n",
            "Model 2 - Epoch 50/100 - Loss: 1.0971\n",
            "Model 3 - Epoch 50/100 - Loss: 1.2814\n",
            "Model 1 - Epoch 51/100 - Loss: 1.2738\n",
            "Model 2 - Epoch 51/100 - Loss: 1.1378\n",
            "Model 3 - Epoch 51/100 - Loss: 1.2886\n",
            "Model 1 - Epoch 52/100 - Loss: 1.3358\n",
            "Model 2 - Epoch 52/100 - Loss: 1.1101\n",
            "Model 3 - Epoch 52/100 - Loss: 1.2731\n",
            "Model 1 - Epoch 53/100 - Loss: 1.3395\n",
            "Model 2 - Epoch 53/100 - Loss: 1.1080\n",
            "Model 3 - Epoch 53/100 - Loss: 1.2633\n",
            "Model 1 - Epoch 54/100 - Loss: 1.3047\n",
            "Model 2 - Epoch 54/100 - Loss: 1.0902\n",
            "Model 3 - Epoch 54/100 - Loss: 1.2780\n",
            "Model 1 - Epoch 55/100 - Loss: 1.3117\n",
            "Model 2 - Epoch 55/100 - Loss: 1.0708\n",
            "Model 3 - Epoch 55/100 - Loss: 1.2665\n",
            "Model 1 - Epoch 56/100 - Loss: 1.3243\n",
            "Model 2 - Epoch 56/100 - Loss: 1.1125\n",
            "Model 3 - Epoch 56/100 - Loss: 1.2562\n",
            "Model 1 - Epoch 57/100 - Loss: 1.3388\n",
            "Model 2 - Epoch 57/100 - Loss: 1.0994\n",
            "Model 3 - Epoch 57/100 - Loss: 1.2435\n",
            "Model 1 - Epoch 58/100 - Loss: 1.3086\n",
            "Model 2 - Epoch 58/100 - Loss: 1.0834\n",
            "Model 3 - Epoch 58/100 - Loss: 1.2404\n",
            "Model 1 - Epoch 59/100 - Loss: 1.3155\n",
            "Model 2 - Epoch 59/100 - Loss: 1.0644\n",
            "Model 3 - Epoch 59/100 - Loss: 1.2556\n",
            "Model 1 - Epoch 60/100 - Loss: 1.3187\n",
            "Model 2 - Epoch 60/100 - Loss: 1.0642\n",
            "Model 3 - Epoch 60/100 - Loss: 1.2230\n",
            "Model 1 - Epoch 61/100 - Loss: 1.3157\n",
            "Model 2 - Epoch 61/100 - Loss: 1.0571\n",
            "Model 3 - Epoch 61/100 - Loss: 1.2242\n",
            "Model 1 - Epoch 62/100 - Loss: 1.2908\n",
            "Model 2 - Epoch 62/100 - Loss: 1.0503\n",
            "Model 3 - Epoch 62/100 - Loss: 1.2126\n",
            "Model 1 - Epoch 63/100 - Loss: 1.2949\n",
            "Model 2 - Epoch 63/100 - Loss: 1.0395\n",
            "Model 3 - Epoch 63/100 - Loss: 1.1933\n",
            "Model 1 - Epoch 64/100 - Loss: 1.3021\n",
            "Model 2 - Epoch 64/100 - Loss: 1.0385\n",
            "Model 3 - Epoch 64/100 - Loss: 1.1907\n",
            "Model 1 - Epoch 65/100 - Loss: 1.2975\n",
            "Model 2 - Epoch 65/100 - Loss: 1.0402\n",
            "Model 3 - Epoch 65/100 - Loss: 1.2001\n",
            "Model 1 - Epoch 66/100 - Loss: 1.2865\n",
            "Model 2 - Epoch 66/100 - Loss: 1.0224\n",
            "Model 3 - Epoch 66/100 - Loss: 1.1860\n",
            "Model 1 - Epoch 67/100 - Loss: 1.2763\n",
            "Model 2 - Epoch 67/100 - Loss: 1.0273\n",
            "Model 3 - Epoch 67/100 - Loss: 1.1939\n",
            "Model 1 - Epoch 68/100 - Loss: 1.2873\n",
            "Model 2 - Epoch 68/100 - Loss: 1.0307\n",
            "Model 3 - Epoch 68/100 - Loss: 1.1677\n",
            "Model 1 - Epoch 69/100 - Loss: 1.2794\n",
            "Model 2 - Epoch 69/100 - Loss: 0.9963\n",
            "Model 3 - Epoch 69/100 - Loss: 1.1697\n",
            "Model 1 - Epoch 70/100 - Loss: 1.2674\n",
            "Model 2 - Epoch 70/100 - Loss: 0.9992\n",
            "Model 3 - Epoch 70/100 - Loss: 1.1527\n",
            "Model 1 - Epoch 71/100 - Loss: 1.2756\n",
            "Model 2 - Epoch 71/100 - Loss: 0.9859\n",
            "Model 3 - Epoch 71/100 - Loss: 1.1704\n",
            "Model 1 - Epoch 72/100 - Loss: 1.2598\n",
            "Model 2 - Epoch 72/100 - Loss: 0.9571\n",
            "Model 3 - Epoch 72/100 - Loss: 1.1752\n",
            "Model 1 - Epoch 73/100 - Loss: 1.2572\n",
            "Model 2 - Epoch 73/100 - Loss: 0.9769\n",
            "Model 3 - Epoch 73/100 - Loss: 1.1505\n",
            "Model 1 - Epoch 74/100 - Loss: 1.2601\n",
            "Model 2 - Epoch 74/100 - Loss: 0.9787\n",
            "Model 3 - Epoch 74/100 - Loss: 1.1418\n",
            "Model 1 - Epoch 75/100 - Loss: 1.2524\n",
            "Model 2 - Epoch 75/100 - Loss: 0.9649\n",
            "Model 3 - Epoch 75/100 - Loss: 1.1430\n",
            "Model 1 - Epoch 76/100 - Loss: 1.2622\n",
            "Model 2 - Epoch 76/100 - Loss: 0.9469\n",
            "Model 3 - Epoch 76/100 - Loss: 1.1338\n",
            "Model 1 - Epoch 77/100 - Loss: 1.2345\n",
            "Model 2 - Epoch 77/100 - Loss: 0.9510\n",
            "Model 3 - Epoch 77/100 - Loss: 1.1259\n",
            "Model 1 - Epoch 78/100 - Loss: 1.2485\n",
            "Model 2 - Epoch 78/100 - Loss: 0.9328\n",
            "Model 3 - Epoch 78/100 - Loss: 1.1203\n",
            "Model 1 - Epoch 79/100 - Loss: 1.2288\n",
            "Model 2 - Epoch 79/100 - Loss: 0.9258\n",
            "Model 3 - Epoch 79/100 - Loss: 1.1087\n",
            "Model 1 - Epoch 80/100 - Loss: 1.2202\n",
            "Model 2 - Epoch 80/100 - Loss: 0.9315\n",
            "Model 3 - Epoch 80/100 - Loss: 1.1017\n",
            "Model 1 - Epoch 81/100 - Loss: 1.2272\n",
            "Model 2 - Epoch 81/100 - Loss: 0.9030\n",
            "Model 3 - Epoch 81/100 - Loss: 1.0951\n",
            "Model 1 - Epoch 82/100 - Loss: 1.2278\n",
            "Model 2 - Epoch 82/100 - Loss: 0.9275\n",
            "Model 3 - Epoch 82/100 - Loss: 1.1064\n",
            "Model 1 - Epoch 83/100 - Loss: 1.2339\n",
            "Model 2 - Epoch 83/100 - Loss: 0.9217\n",
            "Model 3 - Epoch 83/100 - Loss: 1.0928\n",
            "Model 1 - Epoch 84/100 - Loss: 1.2256\n",
            "Model 2 - Epoch 84/100 - Loss: 0.8924\n",
            "Model 3 - Epoch 84/100 - Loss: 1.0858\n",
            "Model 1 - Epoch 85/100 - Loss: 1.2244\n",
            "Model 2 - Epoch 85/100 - Loss: 0.9056\n",
            "Model 3 - Epoch 85/100 - Loss: 1.0870\n",
            "Model 1 - Epoch 86/100 - Loss: 1.2088\n",
            "Model 2 - Epoch 86/100 - Loss: 0.8949\n",
            "Model 3 - Epoch 86/100 - Loss: 1.0819\n",
            "Model 1 - Epoch 87/100 - Loss: 1.2049\n",
            "Model 2 - Epoch 87/100 - Loss: 0.9015\n",
            "Model 3 - Epoch 87/100 - Loss: 1.0779\n",
            "Model 1 - Epoch 88/100 - Loss: 1.2303\n",
            "Model 2 - Epoch 88/100 - Loss: 0.8942\n",
            "Model 3 - Epoch 88/100 - Loss: 1.0592\n",
            "Model 1 - Epoch 89/100 - Loss: 1.2008\n",
            "Model 2 - Epoch 89/100 - Loss: 0.8733\n",
            "Model 3 - Epoch 89/100 - Loss: 1.0591\n",
            "Model 1 - Epoch 90/100 - Loss: 1.1947\n",
            "Model 2 - Epoch 90/100 - Loss: 0.8930\n",
            "Model 3 - Epoch 90/100 - Loss: 1.0670\n",
            "Model 1 - Epoch 91/100 - Loss: 1.1994\n",
            "Model 2 - Epoch 91/100 - Loss: 0.8682\n",
            "Model 3 - Epoch 91/100 - Loss: 1.0573\n",
            "Model 1 - Epoch 92/100 - Loss: 1.1960\n",
            "Model 2 - Epoch 92/100 - Loss: 0.8794\n",
            "Model 3 - Epoch 92/100 - Loss: 1.0625\n",
            "Model 1 - Epoch 93/100 - Loss: 1.1997\n",
            "Model 2 - Epoch 93/100 - Loss: 0.8699\n",
            "Model 3 - Epoch 93/100 - Loss: 1.0603\n",
            "Model 1 - Epoch 94/100 - Loss: 1.2081\n",
            "Model 2 - Epoch 94/100 - Loss: 0.8666\n",
            "Model 3 - Epoch 94/100 - Loss: 1.0619\n",
            "Model 1 - Epoch 95/100 - Loss: 1.2179\n",
            "Model 2 - Epoch 95/100 - Loss: 0.8678\n",
            "Model 3 - Epoch 95/100 - Loss: 1.0612\n",
            "Model 1 - Epoch 96/100 - Loss: 1.2053\n",
            "Model 2 - Epoch 96/100 - Loss: 0.8635\n",
            "Model 3 - Epoch 96/100 - Loss: 1.0422\n",
            "Model 1 - Epoch 97/100 - Loss: 1.1768\n",
            "Model 2 - Epoch 97/100 - Loss: 0.8665\n",
            "Model 3 - Epoch 97/100 - Loss: 1.0569\n",
            "Model 1 - Epoch 98/100 - Loss: 1.1949\n",
            "Model 2 - Epoch 98/100 - Loss: 0.8563\n",
            "Model 3 - Epoch 98/100 - Loss: 1.0622\n",
            "Model 1 - Epoch 99/100 - Loss: 1.1826\n",
            "Model 2 - Epoch 99/100 - Loss: 0.9618\n",
            "Model 3 - Epoch 99/100 - Loss: 1.1380\n",
            "Model 1 - Epoch 100/100 - Loss: 1.1901\n",
            "Model 2 - Epoch 100/100 - Loss: 0.9620\n",
            "Model 3 - Epoch 100/100 - Loss: 1.1405\n",
            "\n",
            "Final Test Accuracies:\n",
            "Model 1: 59.20%\n",
            "Model 2: 69.60%\n",
            "Model 3: 58.80%\n",
            "==================================================\n",
            "Average Model Accuracy: 62.53%\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}